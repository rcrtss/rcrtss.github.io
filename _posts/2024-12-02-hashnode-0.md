---
layout: post
title: comparing performance of various search algorithms in a forward-planning agent
date: 2024-12-02 12:00:00
description: this post is a comparison of the performance of various uninformed and informed search algorithms
tags: ai search
categories: learning
---

After completing the Classical Planning project by coding the mutual exclusion methods for the classes `ActionLayer` and `LiteralLayer`, as well as the heuristic functions for the `PlanningGraph` class, a series of charts were created with the information obtained by running the four **Air cargo problems** on `run_search.py`.

These charts were designed to compare between different search methods and heuristic functions, and analysing how the solution to each problem behaves when adding more actions to the problem.

Three areas are evaluated in this report: complexity, search time and plan length. For each of these, different combinations of search methods, heuristics and problems were considered depending on what is analyzed. The following sections cover the results of each experiment with more detail.

It is important to note that an optimization for efficiency in heuristics such as Max Level can be implemented to reduce times. This was not done for this report since it was optional, but it would be interesting to implement the optimizations and compare the results.

## Search Complexity

The first thing to take into account is that the number of actions does not depend on the search method or heuristic function that is being used. For each problem, this number remains the same throughout every search method and heuristic.

The first comparison was made for problems 1 and 2, for every uninformed search methods and all combination of informed methods with heuristics specified in the project description. *Figure 1* shows a line chart with the number of nodes expanded against the number of actions for a specific search method, for the first two problems (observe that the Y axis is represented with a log scale for better understanding of the behavior).

![Expansions vs Actions for problems 1 and 2](/assets/img/rubric_1_all-1-2-B.png){: style="display: block; margin: 0 auto; max-width: 100%; height: auto;" }

The second comparison (*Figure 2*) was done between the `uniform const search` as the only uninformed search method, and for the `greedy best first search` and the `A*` methods in combination with the `Level Sum` and the `Max Level` heuristics, for problems 3 and 4. Due to the time it takes to run the tests for these two problems, not all the heuristics were considered.

![Expansions vs Actions for problems 3 and 4 (Y-log)](/assets/img/rubric_1_all-3-4.png){: style="display: block; margin: 0 auto; max-width: 100%; height: auto;" }

In contrast to the first two charts, we see in *Figure 3* that analyzing the data with a linear Y scale reveals significant differences in the performance of various search methods.

![Expansions vs Actions for problems 3 and 4 (Y-linear)](/assets/img/rubric_1_3-4-nolog.png){: style="display: block; margin: 0 auto; max-width: 100%; height: auto;" }

### Analysis

In *Figure 1* we can see that for small problems (20 to 72 actions), `greedy best first` algorithm behaves significantly better, in terms of search complexity, with respect to the other methods. More specifically, the `greedy best first` with heuristics `Set Level` and `Level Sum` are the two methods that expand the fewest nodes in both problems 1 and 2. On the other hand, `breadth first` and `uniform cost` searches expand the most nodes when compared to other methods, for these two problems. Lastly, the `depth first` method and the rest of the informed search methods are located in the middleground of the chart (depending on which heuristic they use), but it already shows that they have a significantly higher complexity than the `greedy best first` search methods.

Figures 2 and 3 show how some of these techniques behave when the problem gets more complicated. In *Figure 2* it is clear that, for the considered methods, the one that scales better is the `greedy best first` with `Level Sum` heuristic, followed by the same method with `Max Level` heuristic. On the other hand, the `uniform cost` and `A* with Max Level` methods scale at an exponential rate when the actions increase. *Figure 3* shows quite clearly how these two, but specially the `uniform cost` search method is significantly more inefficient than the rest of the methods in this comparison.

## Search Time

For the next analysis, the time it took for computing each plan, by using different methods, was plotted in two charts. The first of them (*Figure 4*) depicts all methods for problems 1 and 2. The Y axis represents the elapsed time from the start of the algorithm to the end. This is represented in seconds in a logarithmic scale, for the values of `x1 = 20` and `x2 = 72`.

![Search Time (s) vs Actions for problems 1 and 2](/assets/img/rubric_2_all-1-2.png){: style="display: block; margin: 0 auto; max-width: 100%; height: auto;" }

*Figure 5* considers the same methods than for *Figures 2 and 3*, and the problems 1 to 4. Again, the Y axis is in logarithmic scale and represents the time in seconds it took to compute each algorithm.

![Search Time (s) vs Actions for problems 3 and 4](/assets/img/rubric_2_all-3-4.png){: style="display: block; margin: 0 auto; max-width: 100%; height: auto;" }

![Search Time (s) vs Actions for problems 3 and 4 - Uninformed](/assets/img/rubric_2_uninformed-log.png){: style="display: block; margin: 0 auto; max-width: 100%; height: auto;" }

<!-- markdownlint-disable-next-line MD024 -->
### Analysis

By looking at both charts, we can see that the `A*` algorithms, specially with `Set Level` and `Max Level` heuristics are the two most time-expensive algorithms from this comparison. For *Figure 5*, the `Set Level` was not even considered due to the huge amount of time it took to solve problem 4. On the other hand, we can see that the least expensive of all algorithms was `greedy best first` with unmet heuristics, for problems 1 and 2. The uninformed search methods take much less time to compute than the informed methods, as seen in both figures, with the exception of `DFS`, as seen in *Figure 6*.

## Optimality of the solution

Finally, we analyze the optimality of the solutions by looking at the length of the plans outputted by each algorithm for the different problems and different heuristics. For the first two problems, since every method except for `DFS`, had the same number of actions for their solution, *Figure 7* compares only the depth first search against every other method. On the other hand, *Figure 8* takes into account the `uniform cost search` and the other four combinations of the informed searches used in previous sections.

![Plan Length vs Actions for problems 1 and 2](/assets/img/rubric_3_all-1-2-B.png){: style="display: block; margin: 0 auto; max-width: 100%; height: auto;" }

![Plan Length vs Actions for problems 3 and 4](/assets/img/rubric_3_all-3-4.png){: style="display: block; margin: 0 auto; max-width: 100%; height: auto;" }

![Plan Length vs Actions for problems 3 and 4 - Uninformed](/assets/img/rubric_3_uninformed.png){: style="display: block; margin: 0 auto; max-width: 100%; height: auto;" }

<!-- markdownlint-disable-next-line MD024 -->
### Analysis

Looking at figures 7 to 9, we see that the least optimal solution is by far `DFS`. This was already expected due to the nature of this algorithm, and the fact that it does not guarantee to find the shortest path, (not mentioning that it is not even a **complete** search strategy). *Figure 9* shows clearly how DFS finds the least optimal solution when compared even to other uninformed strategies. Finally, in `Figure 8` we can see that the methods that find the most optimal plans for these experiments are `BFS`, `UCS` and `A* with Max Level heuristic`, while other informed methods, such as `greedy best first` with `Level Sum` and with `Max Level` heuristics (which were very optimal in terms of computation time and search complexity), have a weeker performance in terms of optimality in their solutions.

## Additional Questions

Q: **Which algorithm or algorithms would be most appropriate for planning in a very restricted domain (i.e., one that has only a few actions) and needs to operate in real time?**

For problems with few actions, and for a real time application, I would use `greedy best first search` with the `unmet goals` heuristics, since according to the experiments, it behaves much better in terms of computation time than any of the other algorithms that were tested. While for problems that require more actions it may not find the most optimal solution, for small problems it did find an optimal solution, just as the best of the other algorithms.

Q: **Which algorithm or algorithms would be most appropriate for planning in very large domains (e.g., planning delivery routes for all UPS drivers in the U.S. on a given day)**

For large domains, where computation resources may be available, I would probably choose between `A* with level sum or max level`, or an uninformed search like `UCS`. `A*`, specially with max level takes quite some time to compute, specially when the problem gets bigger, but it does find a more optimal solution, while not expanding as many nodes as BFS or UCS. UCS takes much less time to compute and finds an optimal solution as well, but with the drawback of expanding nodes that may not be relevant. Although, if an optimal solution is not required, I would go for a `greedy best first search` with level sum, which has the best of both worlds with the drawback of not finding an optimal solution.

Q: **Which algorithm or algorithms would be most appropriate for planning problems where it is important to find only optimal plans?**

In this case I would go for a simple `UCS`. While `A* with Max Level` could also be an option, I cannot find a justification on why to use it instead of UCS (or even BFS), unless the problem had some boundaries or a more complex path to find the solution. UCS finds the optimal path and behaves well with small and large domains.
